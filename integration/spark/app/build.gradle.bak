plugins {
    id("java-library")
    id("pmd")
    id("com.diffplug.spotless")
    id("io.freefair.lombok")
    id("com.github.johnrengelman.shadow")
    id("de.undercouch.download")
    id("io.openlineage.common-config")
    id("io.openlineage.spark-variant-build")
    id "com.adarshr.test-logger" version "3.2.0"
    id "org.gradle.test-retry" version "1.5.8"
}

configurations {
    spark2.extendsFrom testImplementation
    spark3.extendsFrom testImplementation
    spark31.extendsFrom testImplementation
    spark32.extendsFrom testImplementation
    spark33.extendsFrom testImplementation
    spark34.extendsFrom testImplementation
    spark35.extendsFrom testImplementation
    pysparkContainerOnly
}

sparkVariants {
    add("Spark 3.5.x", "3.5.0", ["2.12", "2.13"], null, null, "2.13.0-spark_3.4", "0.29.0", "3.3.4")
    add("Spark 3.4.x", "3.4.2", ["2.12", "2.13"], "1.3.0", "2.4.0", "2.13.0-spark_3.4", "0.29.0", "3.3.4")
    add("Spark 3.3.x", "3.3.4", ["2.12", "2.13"], "0.14.0", "2.1.0", "2.11.0-spark_3.3", "0.29.0", "3.3.4")
    add("Spark 3.2.x", "3.2.4", ["2.12", "2.13"], "0.14.0", "1.1.0", "2.11.0-spark_3.2", "0.29.0", "3.3.4")
    add("Spark 2.4.x", "2.4.8", ["2.12"], null, null, "2.9.3-spark_2.4", "0.29.0", "2.10.2")

    dockerManifests = project.file("docker/manifests.json")
    dockerfileTemplate = project.file("docker/Dockerfile.template")
}

tasks.register("printConfigurations") {
    doLast {
        configurations.each { println it.name }
    }
}

archivesBaseName = "openlineage-spark-app"

ext {
    assertjVersion = "3.25.1"
    bigqueryVersion = "0.29.0"
    junit5Version = "5.10.1"
    mockitoVersion = "4.11.0"
    postgresqlVersion = "42.7.1"
    testcontainersVersion = "1.19.3"

    sparkVersion = project.findProperty("spark.version")
    shortVersion = sparkVersion.substring(0, 3)
    scalaBinaryVersion = project.findProperty("scala.binary.version")
    formattedScalaVersion = scalaBinaryVersion.replace(".", "")

    versionsMap = [
            "3.5": ["module": "spark35", "scala": "2.12", "delta": "NA", "gcs": "hadoop3-2.2.9", "snowflake": "2.13.0-spark_3.4", "iceberg": "NA", "hadoopclient": "3.3.4"],
            "3.4": ["module": "spark34", "scala": "2.12", "delta": "2.4.0", "gcs": "hadoop3-2.2.9", "snowflake": "2.13.0-spark_3.4", "iceberg": "iceberg-spark-runtime-3.4_2.12:1.3.0", "hadoopclient": "3.3.4"],
            "3.3": ["module": "spark33", "scala": "2.12", "delta": "2.1.0", "gcs": "hadoop3-2.2.9", "snowflake": "2.11.0-spark_3.3", "iceberg": "iceberg-spark-runtime-3.3_2.12:0.14.0", "hadoopclient": "3.3.4"],
            "3.2": ["module": "spark32", "scala": "2.12", "delta": "1.1.0", "gcs": "hadoop3-2.2.9", "snowflake": "2.11.0-spark_3.2", "iceberg": "iceberg-spark-runtime-3.2_2.12:0.14.0", "hadoopclient": "3.3.4"],
            "2.4": ["module": "spark2", "scala": "2.11", "delta": "NA", "gcs": "hadoop2-2.2.9", "snowflake": "2.9.3-spark_2.4", "iceberg": "NA", "hadoopclient": "2.10.2"]
    ]
    versions = versionsMap[shortVersion]
}

// This workaround is needed because the version of Snappy that Spark 2.4.x runs with,
// cannot run on Apple Silicon. It fails with:
// org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] no native library is found for os.name=Mac and os.arch=aarch64
configurations.configureEach {
    resolutionStrategy.eachDependency {
        if (requested.group == "org.xerial.snappy" && requested.name == "snappy-java") {
            useVersion("[1.1.8.4,)")
        }
    }
}

dependencies {
    testImplementation("org.apache.spark:spark-mllib_${versions.scala}:${sparkVersion}") {
        exclude group: "org.glassfish.jaxb", module: "jaxb-runtime"
        exclude group: "org.apache.spark", module: "spark-graphx_${versions.scala}"
    }

    testImplementation("commons-beanutils:commons-beanutils:1.9.4")

    if (versions.module != "spark2") {
        testImplementation("org.apache.spark:spark-hadoop-cloud_${versions.scala}:${sparkVersion}") {
            exclude group: "com.fasterxml.jackson.core"
            exclude group: "org.apache.hadoop", module: "hadoop-azure"
            exclude group: "org.apache.hadoop", module: "hadoop-openstack"
        }
    }

    pysparkContainerOnly("com.google.cloud.bigdataoss:gcs-connector:${versions.gcs}:shaded") {
        exclude group: "com.google.guava", module: "guava"
    }

    pysparkContainerOnly("com.google.guava:guava:30.1-jre")

    if (versions.delta != "NA") {
        testImplementation("io.delta:delta-core_${versions.scala}:${versions.delta}")
    }

    if (versions.iceberg != "NA") {
        testImplementation("org.apache.iceberg:${versions.iceberg}")
    }

    if (versions.module != "spark2") {
        testImplementation("com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.9") {
            exclude group: "com.google.guava", module: "guava"
        }
    }
}

def commonTestConfiguration = {
    forkEvery 1
    maxParallelForks 5
    testLogging {
        events "passed", "skipped", "failed"
        showStandardStreams = true
    }
    systemProperties = [
            "junit.platform.output.capture.stdout": "true",
            "junit.platform.output.capture.stderr": "true",
            "spark.version"                       : sparkVersion,
            "openlineage.spark.jar"               : "${archivesBaseName}-${project.version}.jar",
            "kafka.package.version"               : "org.apache.spark:spark-sql-kafka-0-10_${versions.scala}:${sparkVersion}",
            "mockserver.logLevel"                 : "ERROR"
    ]

    classpath = project.sourceSets.test.runtimeClasspath
}

tasks.withType(Test).configureEach {
    failFast = true
}

tasks.register("nonParallelTest", Test) {
    dependsOn(tasks.named("shadowJar"))
    group = "verification"
    description = "Runs tests that cannot be run in parallel. For example test suites that require metastore_db"

    maxParallelForks = 1

    useJUnitPlatform {
        includeTags("nonParallelTest")
    }
}

tasks.register("beforeShadowJarTest", Test) {
    doFirst {
        delete("derby.log", "metastore_db", "spark-warehouse")
    }

    dependsOn(tasks.named("testClasses"))
    group = "verification"
    description = "Tests that depend that have to be run before shadowJar"
    useJUnitPlatform {
        includeTags("beforeShadowJarTest")
    }
}

// wrócić do jednego test z jakąś metodą zwracającą konfigurację żeby spark3 działał
test {
    dependsOn nonParallelTest
    useJUnitPlatform { i ->
        excludeTags("integration-test", "nonParallelTest", "beforeShadowJarTest")
        if (versions.delta == "NA") {
            excludeTags "delta"
        }
        if (versions.iceberg == "NA") {
            excludeTags "iceberg"
        }
    }
    configure commonTestConfiguration
    classpath = project.sourceSets.test.runtimeClasspath + configurations."${versions.module}"
}

tasks.register("copyDependencies", Copy) {
    dependsOn(tasks.named("shadowJar"), tasks.named("jar"))
    // delete the dependencies directory so we don"t accidentally mix Spark 2 and Spark 3 dependencies
    duplicatesStrategy = DuplicatesStrategy.EXCLUDE
    delete layout.buildDirectory.dir("dependencies")
    def config = configurations."${versions.module}"
    from config.getFiles() + configurations.pysparkContainerOnly.getFiles()
    include "*.jar"
    into layout.buildDirectory.dir("dependencies")
}

tasks.register("integrationTest", Test) {
    dependsOn(tasks.named("shadowJar"), tasks.named("copyDependencies"))
    retry {
        boolean isCiServer = System.getenv().containsKey("CI")
        if (isCiServer) {
            maxRetries = 3
            maxFailures = 3
        }
    }
    configure commonTestConfiguration
    useJUnitPlatform {
        includeTags "integration-test"
        excludeTags "databricks"
        if (versions.delta == "NA") {
            excludeTags "delta"
        }
        if (versions.iceberg == "NA") {
            excludeTags "iceberg"
        }
    }
}

tasks.register("databricksIntegrationTest", Test) {
    dependsOn(tasks.named("shadowJar"), tasks.named("copyDependencies"))
    retry {
        boolean isCiServer = System.getenv().containsKey("CI")
        if (isCiServer) {
            maxRetries = 3
            maxFailures = 3
        }
    }
    configure commonTestConfiguration
    useJUnitPlatform {
        includeTags "databricks"
    }
}

assemble {
    dependsOn shadowJar
}

shadowJar {
    dependsOn beforeShadowJarTest

    minimize() {
        exclude(project(":shared"))
        exclude(project(":spark2"))
        exclude(project(":spark3"))
        exclude(project(":spark31"))
        exclude(project(":spark32"))
        exclude(project(":spark33"))
        exclude(project(":spark34"))
        exclude(project(":spark35"))
    }
    archiveClassifier = ""
    // avoid conflict with any client version of that lib
    relocate "com.github.ok2c.hc5", "io.openlineage.spark.shaded.com.github.ok2c.hc5"
    relocate "org.apache.httpcomponents.client5", "io.openlineage.spark.shaded.org.apache.httpcomponents.client5"
    relocate "javassist", "io.openlineage.spark.shaded.javassist"
    relocate "org.apache.hc", "io.openlineage.spark.shaded.org.apache.hc"
    relocate "org.apache.commons.codec", "io.openlineage.spark.shaded.org.apache.commons.codec"
    relocate "org.apache.commons.lang3", "io.openlineage.spark.shaded.org.apache.commons.lang3"
    relocate "org.apache.commons.beanutils", "io.openlineage.spark.shaded.org.apache.commons.beanutils"
    relocate "org.apache.http", "io.openlineage.spark.shaded.org.apache.http"
    relocate "org.yaml.snakeyaml", "io.openlineage.spark.shaded.org.yaml.snakeyaml"
    relocate "org.slf4j", "io.openlineage.spark.shaded.org.slf4j"
    relocate("com.fasterxml.jackson", "io.openlineage.spark.shaded.com.fasterxml.jackson") {
        exclude "com.fasterxml.jackson.annotation.JsonIgnore"
        exclude "com.fasterxml.jackson.annotation.JsonIgnoreProperties"
        exclude "com.fasterxml.jackson.annotation.JsonIgnoreType"
    }
    manifest {
        attributes(
                "Created-By": "Gradle ${gradle.gradleVersion}",
                "Built-By": System.getProperty("user.name"),
                "Build-Jdk": System.getProperty("java.version"),
                "Implementation-Title": project.name,
                "Implementation-Version": project.version
        )
    }
    zip64 true
}

tasks.register("createVersionProperties") {
    dependsOn processResources
    doLast {
        File dir = new File("$buildDir/resources/main/io/openlineage/spark/agent/")
        dir.mkdirs();
        new File("$buildDir/resources/main/io/openlineage/spark/agent/version.properties").withWriter { w ->
            Properties p = new Properties()
            p["version"] = project.version.toString()
            p.store w, null
        }
    }
}

classes {
    dependsOn createVersionProperties
}

tasks.named("pmdTest") {
    mustRunAfter("shadowJar")
}
