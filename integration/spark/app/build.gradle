plugins {
    id("java-library")
    id("pmd")
    id("com.diffplug.spotless")
    id("io.freefair.lombok")
    id("com.github.johnrengelman.shadow")
    id("de.undercouch.download")
    id("io.openlineage.common-config")
    id("io.openlineage.spark-builds")
    id "com.adarshr.test-logger" version "3.2.0"
    id "org.gradle.test-retry" version "1.5.8"
}

configurations {
    spark2.extendsFrom testImplementation
    spark3.extendsFrom testImplementation
    spark31.extendsFrom testImplementation
    spark32.extendsFrom testImplementation
    spark33.extendsFrom testImplementation
    spark34.extendsFrom testImplementation
    spark35.extendsFrom testImplementation
    pysparkContainerOnly
}

sparkVersions {
    add("Spark 3.5.x", "3.5.0", ["2.12", "2.13"], null, null, "2.13.0-spark_3.4", "0.29.0", "3.3.4")
    add("Spark 3.4.x", "3.4.2", ["2.12", "2.13"], "1.3.0", "2.4.0", "2.13.0-spark_3.4", "0.29.0", "3.3.4")
    add("Spark 3.3.x", "3.3.4", ["2.12", "2.13"], "0.14.0", "2.1.0", "2.11.0-spark_3.3", "0.29.0", "3.3.4")
    add("Spark 3.2.x", "3.2.4", ["2.12", "2.13"], "0.14.0", "1.1.0", "2.11.0-spark_3.2", "0.29.0", "3.3.4")
    add("Spark 2.4.x", "2.4.8", ["2.12"], null, null, "2.9.3-spark_2.4", "0.29.0", "2.10.2")
}

archivesBaseName = "openlineage-spark-app"

ext {
    assertjVersion = "3.25.1"
    bigqueryVersion = "0.29.0"
    junit5Version = "5.10.1"
    mockitoVersion = "4.11.0"
    postgresqlVersion = "42.7.1"
    testcontainersVersion = "1.19.3"

    sparkVersion = project.findProperty("spark.version")
    shortVersion = sparkVersion.substring(0, 3)
    scalaBinaryVersion = project.findProperty("scala.binary.version")
    formattedScalaVersion = scalaBinaryVersion.replace(".", "")

    versionsMap = [
            "3.5": ["module": "spark35", "scala": "2.12", "delta": "NA", "gcs": "hadoop3-2.2.9", "iceberg": "NA", "hadoopclient": "3.3.4"],
            "3.4": ["module": "spark34", "scala": "2.12", "delta": "2.4.0", "gcs": "hadoop3-2.2.9", "iceberg": "iceberg-spark-runtime-3.4_2.12:1.3.0", "hadoopclient": "3.3.4"],
            "3.3": ["module": "spark33", "scala": "2.12", "delta": "2.1.0", "gcs": "hadoop3-2.2.9", "iceberg": "iceberg-spark-runtime-3.3_2.12:0.14.0", "hadoopclient": "3.3.4"],
            "3.2": ["module": "spark32", "scala": "2.12", "delta": "1.1.0", "gcs": "hadoop3-2.2.9", "iceberg": "iceberg-spark-runtime-3.2_2.12:0.14.0", "hadoopclient": "3.3.4"],
            "3.1": ["module": "spark31", "scala": "2.12", "delta": "1.0.0", "gcs": "hadoop3-2.2.9", "iceberg": "iceberg-spark-runtime-3.1_2.12:0.13.0", "hadoopclient": "3.3.4"],
            "2.4": ["module": "spark2", "scala": "2.11", "delta": "NA", "gcs": "hadoop2-2.2.9", "iceberg": "NA", "hadoopclient": "2.10.2"]
    ]
    versions = versionsMap[shortVersion]
}

// This workaround is needed because the version of Snappy that Spark 2.4.x runs with,
// cannot run on Apple Silicon. It fails with:
// org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] no native library is found for os.name=Mac and os.arch=aarch64
configurations.configureEach {
    resolutionStrategy.eachDependency {
        if (requested.group == "org.xerial.snappy" && requested.name == "snappy-java") {
            useVersion("[1.1.8.4,)")
        }
    }
}

dependencies {
    implementation(project(path: ":shared", configuration: "scala212RuntimeElements"))
    implementation(project(path: ":spark2", configuration: "scala212RuntimeElements"))
    implementation(project(path: ":spark3", configuration: "scala212RuntimeElements"))
    implementation(project(path: ":spark31", configuration: "scala212RuntimeElements"))
    implementation(project(path: ":spark32", configuration: "scala212RuntimeElements"))
    implementation(project(path: ":spark33", configuration: "scala212RuntimeElements"))
    implementation(project(path: ":spark34", configuration: "scala212RuntimeElements"))
    implementation(project(path: ":spark35", configuration: "scala212RuntimeElements"))
    implementation "org.apache.httpcomponents.client5:httpclient5:5.3"

    compileOnly "org.apache.spark:spark-core_${versions.scala}:${sparkVersion}"
    compileOnly "org.apache.spark:spark-sql_${versions.scala}:${sparkVersion}"
    compileOnly("com.google.cloud.spark:spark-bigquery-with-dependencies_${versions.scala}:${bigqueryVersion}") {
        exclude group: "com.fasterxml.jackson.core"
        exclude group: "com.fasterxml.jackson.module"
        exclude group: "com.sun.jmx"
        exclude group: "com.sun.jdmk"
        exclude group: "javax.jms"
    }

    testImplementation(platform("org.junit:junit-bom:5.10.1"))
    testImplementation("org.junit.jupiter:junit-jupiter:${junit5Version}")
    testImplementation("org.junit.jupiter:junit-jupiter-params:${junit5Version}")

    testImplementation("org.postgresql:postgresql:${postgresqlVersion}")
    testImplementation("org.hamcrest:hamcrest-library:2.2")
    testImplementation("org.xerial:sqlite-jdbc:3.45.0.0")
    testImplementation("org.testcontainers:junit-jupiter:${testcontainersVersion}")
    testImplementation("org.testcontainers:postgresql:${testcontainersVersion}")
    testImplementation("org.testcontainers:mockserver:${testcontainersVersion}")
    testImplementation("org.testcontainers:kafka:${testcontainersVersion}")

    testImplementation("org.apache.spark:spark-core_${versions.scala}:${sparkVersion}")
    testImplementation("org.apache.spark:spark-sql_${versions.scala}:${sparkVersion}")
    testImplementation("org.apache.spark:spark-hive_${versions.scala}:${sparkVersion}")
    testImplementation("org.apache.spark:spark-mllib_${versions.scala}:${sparkVersion}") {
        exclude group: "org.glassfish.jaxb", module: "jaxb-runtime"
        exclude group: "org.apache.spark", module: "spark-graphx_${versions.scala}"
    }
    testImplementation("commons-beanutils:commons-beanutils:1.9.4")
    if (versions.module != "spark2") {
        testImplementation("org.apache.spark:spark-hadoop-cloud_${versions.scala}:${sparkVersion}") {
            exclude group: "com.fasterxml.jackson.core"
            exclude group: "org.apache.hadoop", module: "hadoop-azure"
            exclude group: "org.apache.hadoop", module: "hadoop-openstack"
        }
    }

    testImplementation("org.apache.spark:spark-sql-kafka-0-10_${versions.scala}:${sparkVersion}")
    testImplementation("com.google.cloud.spark:spark-bigquery-with-dependencies_${versions.scala}:${bigqueryVersion}") {
        exclude group: "com.fasterxml.jackson.core"
        exclude group: "com.fasterxml.jackson.module"
    }
    testImplementation("org.apache.spark:spark-sql_${versions.scala}:${sparkVersion}")
    testImplementation("com.databricks:databricks-sdk-java:0.4.0") {
        exclude group: "com.fasterxml.jackson.core"
        exclude group: "com.fasterxml.jackson.module"
    }

    testImplementation("org.apache.hadoop:hadoop-client:${versions.hadoopclient}") {
        exclude group: "org.apache.hadoop", module: "hadoop-hdfs-client"
        exclude group: "org.apache.hadoop", module: "hadoop-client"
        exclude group: "org.apache.hadoop", module: "hadoop-mapreduce-client-core"
        exclude group: "org.apache.hadoop", module: "hadoop-yarn-common"
        exclude group: "com.fasterxml.jackson.core"
    }
    testImplementation("org.mock-server:mockserver-netty:5.14.0:shaded") {
        exclude group: "com.google.guava", module: "guava"
        exclude group: "com.fasterxml.jackson.core"
        exclude group: "com.fasterxml.jackson.datatype"
        exclude group: "com.fasterxml.jackson.dataformat"
        exclude group: "org.mock-server.mockserver-client-java"
    }

    testImplementation("org.awaitility:awaitility:4.2.0")
    testImplementation("org.assertj:assertj-core:${assertjVersion}")
    testImplementation("org.mockito:mockito-core:${mockitoVersion}")
    testImplementation("org.mockito:mockito-inline:${mockitoVersion}")
    testImplementation("org.mockito:mockito-junit-jupiter:${mockitoVersion}")

    pysparkContainerOnly("com.google.cloud.bigdataoss:gcs-connector:${versions.gcs}:shaded") {
        exclude group: "com.google.guava", module: "guava"
    }
    pysparkContainerOnly("com.google.guava:guava:30.1-jre")

    if (versions.delta != "NA") {
        testImplementation("io.delta:delta-core_${versions.scala}:${versions.delta}")
    }

    if (versions.iceberg != "NA") {
        testImplementation("org.apache.iceberg:${versions.iceberg}")
    }

    if (versions.module != "spark2") {
        testImplementation("com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.9") {
            exclude group: "com.google.guava", module: "guava"
        }
    }
}

def commonTestConfiguration = {
    forkEvery 1
    maxParallelForks 5
    testLogging {
        events "passed", "skipped", "failed"
        showStandardStreams = true
    }
    systemProperties = [
            "junit.platform.output.capture.stdout": "true",
            "junit.platform.output.capture.stderr": "true",
            "spark.version"                       : sparkVersion,
            "openlineage.spark.jar"               : "${archivesBaseName}-${project.version}.jar",
            "kafka.package.version"               : "org.apache.spark:spark-sql-kafka-0-10_${versions.scala}:${sparkVersion}",
            "mockserver.logLevel"                 : "ERROR"
    ]

    classpath = project.sourceSets.test.runtimeClasspath
}

tasks.withType(Test).configureEach {
    failFast = true
}

tasks.register("nonParallelTest", Test) {
    dependsOn(tasks.named("shadowJar"))
    group = "verification"
    description = "Runs tests that cannot be run in parallel. For example test suites that require metastore_db"

    maxParallelForks = 1

    useJUnitPlatform {
        includeTags("nonParallelTest")
    }
}

tasks.register("beforeShadowJarTest", Test) {
    doFirst {
        delete("derby.log", "metastore_db", "spark-warehouse")
    }

    dependsOn(tasks.named("testClasses"))
    group = "verification"
    description = "Tests that depend that have to be run before shadowJar"
    useJUnitPlatform {
        includeTags("beforeShadowJarTest")
    }
}

// wrócić do jednego test z jakąś metodą zwracającą konfigurację żeby spark3 działał
test {
    dependsOn(tasks.named("nonParallelTest"))
    useJUnitPlatform { i ->
        excludeTags("integration-test", "nonParallelTest", "beforeShadowJarTest")
        if (versions.delta == "NA") {
            excludeTags "delta"
        }
        if (versions.iceberg == "NA") {
            excludeTags "iceberg"
        }
    }
    configure commonTestConfiguration
    classpath = project.sourceSets.test.runtimeClasspath + configurations."${versions.module}"
}

tasks.register("copyDependencies", Copy) {
    dependsOn(tasks.named("shadowJar"), tasks.named("jar"))
    // delete the dependencies directory so we don"t accidentally mix Spark 2 and Spark 3 dependencies
    duplicatesStrategy = DuplicatesStrategy.EXCLUDE
    delete layout.buildDirectory.dir("dependencies")
    def config = configurations."${versions.module}"
    from config.getFiles() + configurations.pysparkContainerOnly.getFiles()
    include "*.jar"
    into layout.buildDirectory.dir("dependencies")
}

tasks.register("integrationTest", Test) {
    dependsOn(tasks.named("shadowJar"), tasks.named("copyDependencies"))
    retry {
        boolean isCiServer = System.getenv().containsKey("CI")
        if (isCiServer) {
            maxRetries = 3
            maxFailures = 3
        }
    }
    configure commonTestConfiguration
    useJUnitPlatform {
        includeTags "integration-test"
        excludeTags "databricks"
        if (!sparkVersion.startsWith("3")) {
            excludeTags "spark3"
        }
        if (versions.delta == "NA") {
            excludeTags "delta"
        }
        if (versions.iceberg == "NA") {
            excludeTags "iceberg"
        }
    }
}

tasks.register("databricksIntegrationTest", Test) {
    dependsOn(tasks.named("shadowJar"), tasks.named("copyDependencies"))
    retry {
        boolean isCiServer = System.getenv().containsKey("CI")
        if (isCiServer) {
            maxRetries = 3
            maxFailures = 3
        }
    }
    configure commonTestConfiguration
    useJUnitPlatform {
        includeTags "databricks"
    }
}

assemble {
    dependsOn shadowJar
}

shadowJar {
    dependsOn(tasks.named("beforeShadowJarTest"))

    minimize() {
        exclude(project(":shared"))
        exclude(project(":spark2"))
        exclude(project(":spark3"))
        exclude(project(":spark31"))
        exclude(project(":spark32"))
        exclude(project(":spark33"))
        exclude(project(":spark34"))
        exclude(project(":spark35"))
    }
    archiveClassifier = ""
    // avoid conflict with any client version of that lib
    relocate "com.github.ok2c.hc5", "io.openlineage.spark.shaded.com.github.ok2c.hc5"
    relocate "org.apache.httpcomponents.client5", "io.openlineage.spark.shaded.org.apache.httpcomponents.client5"
    relocate "javassist", "io.openlineage.spark.shaded.javassist"
    relocate "org.apache.hc", "io.openlineage.spark.shaded.org.apache.hc"
    relocate "org.apache.commons.codec", "io.openlineage.spark.shaded.org.apache.commons.codec"
    relocate "org.apache.commons.lang3", "io.openlineage.spark.shaded.org.apache.commons.lang3"
    relocate "org.apache.commons.beanutils", "io.openlineage.spark.shaded.org.apache.commons.beanutils"
    relocate "org.apache.http", "io.openlineage.spark.shaded.org.apache.http"
    relocate "org.yaml.snakeyaml", "io.openlineage.spark.shaded.org.yaml.snakeyaml"
    relocate "org.slf4j", "io.openlineage.spark.shaded.org.slf4j"
    relocate("com.fasterxml.jackson", "io.openlineage.spark.shaded.com.fasterxml.jackson") {
        exclude "com.fasterxml.jackson.annotation.JsonIgnore"
        exclude "com.fasterxml.jackson.annotation.JsonIgnoreProperties"
        exclude "com.fasterxml.jackson.annotation.JsonIgnoreType"
    }
    manifest {
        attributes(
                "Created-By": "Gradle ${gradle.gradleVersion}",
                "Built-By": System.getProperty("user.name"),
                "Build-Jdk": System.getProperty("java.version"),
                "Implementation-Title": project.name,
                "Implementation-Version": project.version
        )
    }
    zip64 true
}

tasks.register("createVersionProperties") {
    dependsOn processResources
    doLast {
        File dir = new File("$buildDir/resources/main/io/openlineage/spark/agent/")
        dir.mkdirs();
        new File("$buildDir/resources/main/io/openlineage/spark/agent/version.properties").withWriter { w ->
            Properties p = new Properties()
            p["version"] = project.version.toString()
            p.store w, null
        }
    }
}

classes {
    dependsOn(tasks.named("createVersionProperties"))
}

tasks.named("pmdTest") {
    mustRunAfter("shadowJar")
}
