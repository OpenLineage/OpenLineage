import org.apache.tools.ant.filters.ReplaceTokens
import groovy.io.FileType

import java.nio.file.Files


plugins {
    id 'java'
    id 'java-library'
    id 'java-test-fixtures'
    id 'com.diffplug.spotless' version '6.12.0'
    id "com.adarshr.test-logger" version "3.2.0"
    id "org.gradle.test-retry" version "1.5.3"
    id "com.github.johnrengelman.shadow" version "7.1.2"
    id "pmd"
}

pmd {
    consoleOutput = true
    toolVersion = "6.46.0"
    rulesMinimumPriority = 5
    ruleSetFiles = rootProject.files("pmd-openlineage.xml")
    ruleSets = []
    ignoreFailures = true
}

pmdMain {
    reports {
        html.required = true
    }
}

repositories {
    mavenLocal()
    mavenCentral()
    maven {
        url = 'https://astronomer.jfrog.io/artifactory/maven-public-libs-snapshot'
    }
}

configurations {
    lombok
    spark2.extendsFrom testImplementation
    spark3.extendsFrom testImplementation
    spark32.extendsFrom testImplementation
    spark33.extendsFrom testImplementation
    pysparkContainerOnly
}

archivesBaseName='openlineage-spark-app'


ext {
    bigqueryVersion = '0.26.0'
    sparkVersion = project.getProperty('spark.version')
    postgresqlVersion = '42.2.19'
    mockitoVersion = '3.11.2'
    testcontainersVersion = '1.17.6'
    shortVersion = sparkVersion.substring(0,3)

    versionsMap = [
            "3.4": ["module": "spark33", "scala": "2.12", "delta": "2.4.0", "gcs": "hadoop3-2.2.9", "snowflake": "2.11.0-spark_3.3", "iceberg": "iceberg-spark-runtime-3.4_2.12:1.3.0", "hadoopclient": "3.3.4"],
            "3.3": ["module": "spark33", "scala": "2.12", "delta": "2.1.0", "gcs": "hadoop3-2.2.9", "snowflake": "2.11.0-spark_3.3", "iceberg": "iceberg-spark-runtime-3.3_2.12:0.14.0", "hadoopclient": "3.3.4"],
            "3.2": ["module": "spark32", "scala": "2.12", "delta": "1.1.0", "gcs": "hadoop3-2.2.9", "snowflake": "2.11.0-spark_3.2", "iceberg": "iceberg-spark-runtime-3.2_2.12:0.14.0", "hadoopclient": "3.3.4"],
            "3.1": ["module": "spark3",  "scala": "2.12", "delta": "1.0.0", "gcs": "hadoop3-2.2.9", "snowflake": "2.11.0-spark_3.1", "iceberg": "iceberg-spark-runtime-3.1_2.12:0.13.0", "hadoopclient": "3.3.4"],
            "2.4": ["module": "spark2",  "scala": "2.11", "delta": "NA",    "gcs": "hadoop2-2.2.9", "snowflake": "2.9.3-spark_2.4",  "iceberg": "NA", "hadoopclient": "2.10.2"]
    ]
    versions = versionsMap[shortVersion]
}

configurations.all { // https://github.com/apache/spark/pull/38355 - can be remove for Spark 3.3.2
    resolutionStrategy {
        // https://github.com/FasterXML/jackson-databind/issues/3627
        force "com.fasterxml.jackson:jackson-bom:$jacksonVersion"
    }
}

dependencies {
    compileOnly "org.projectlombok:lombok:${lombokVersion}"
    annotationProcessor "org.projectlombok:lombok:${lombokVersion}"
    testCompileOnly "org.projectlombok:lombok:${lombokVersion}"
    testAnnotationProcessor "org.projectlombok:lombok:${lombokVersion}"

    implementation(project(path: ":shared"))
    implementation(project(path: ":spark2"))
    implementation(project(path: ":spark3"))
    implementation(project(path: ":spark32"))
    implementation(project(path: ":spark33"))
    implementation 'org.apache.httpcomponents.client5:httpclient5:5.2.1'


    compileOnly "org.apache.spark:spark-core_${versions.scala}:${sparkVersion}"
    compileOnly "org.apache.spark:spark-sql_${versions.scala}:${sparkVersion}"
    compileOnly ("com.google.cloud.spark:spark-bigquery-with-dependencies_${versions.scala}:${bigqueryVersion}") {
        exclude group: 'com.fasterxml.jackson.core'
        exclude group: 'com.fasterxml.jackson.module'
        exclude group: 'com.sun.jmx'
        exclude group: 'com.sun.jdmk'
        exclude group: 'javax.jms'
    }

    compileOnly ("net.snowflake:spark-snowflake_${versions.scala}:${versions.snowflake}") {
        exclude group: 'com.google.guava:guava'
        exclude group: 'org.apache.spark:spark-core_2.11'
        exclude group: 'org.apache.spark:spark-sql_2.11'
        exclude group: 'org.apache.spark:spark-catalyst_2.11'
    }

    testFixturesApi "org.apache.spark:spark-core_${versions.scala}:${sparkVersion}"
    testFixturesApi "org.apache.spark:spark-sql_${versions.scala}:${sparkVersion}"
    testFixturesApi "org.apache.spark:spark-hive_${versions.scala}:${sparkVersion}"
    testFixturesApi 'commons-beanutils:commons-beanutils:1.9.4'


    testFixturesApi ("net.snowflake:spark-snowflake_${versions.scala}:${versions.snowflake}") {
        exclude group: 'com.google.guava:guava'
        exclude group: 'org.apache.spark:spark-core_2.11'
        exclude group: 'org.apache.spark:spark-sql_2.11'
        exclude group: 'org.apache.spark:spark-catalyst_2.11'
    }

    testFixturesApi "org.apache.spark:spark-sql-kafka-0-10_${versions.scala}:${sparkVersion}"
    testFixturesApi("com.google.cloud.spark:spark-bigquery-with-dependencies_${versions.scala}:${bigqueryVersion}") {
        exclude group: 'com.fasterxml.jackson.core'
        exclude group: 'com.fasterxml.jackson.module'
    }
    testFixturesApi("com.databricks:databricks-sdk-java:0.0.1") {
        exclude group: 'com.fasterxml.jackson.core'
        exclude group: 'com.fasterxml.jackson.module'
    }

    testFixturesApi("org.apache.hadoop:hadoop-client:${versions.hadoopclient}") { force=true }
    testFixturesApi("org.mock-server:mockserver-netty:5.14.0:shaded") {
        exclude group: 'com.google.guava', module: 'guava'
        exclude group: 'com.fasterxml.jackson.core'
        exclude group: 'com.fasterxml.jackson.datatype'
        exclude group: 'com.fasterxml.jackson.dataformat'
        exclude group: 'org.mock-server.mockserver-client-java'
    }

    pysparkContainerOnly("com.google.cloud.bigdataoss:gcs-connector:${versions.gcs}:shaded"){
        exclude group: 'com.google.guava', module: 'guava'
    }
    pysparkContainerOnly("com.google.guava:guava:30.1-jre")

    if(versions.delta != "NA") { testFixturesApi "io.delta:delta-core_2.12:${versions.delta}" }

    if(versions.iceberg != "NA") { testFixturesApi "org.apache.iceberg:${versions.iceberg}" }

    testImplementation(project(":${versions.module}"))
    if(versions.module != "spark2") {
        testImplementation("com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.9"){
            exclude group: 'com.google.guava', module: 'guava'
        }
        pysparkContainerOnly("com.google.guava:guava:30.1-jre")
    }
    
    
    testImplementation testFixtures(project(":${versions.module}")){
        exclude group: 'org.apache.avro'
    }
    testImplementation testFixtures(project(":shared"))

    lombok  "org.projectlombok:lombok:${lombokVersion}"
    testAnnotationProcessor "org.projectlombok:lombok:${lombokVersion}"
}

def commonTestConfiguration = {
    forkEvery 1
    maxParallelForks 5
    testLogging {
        events "passed", "skipped", "failed"
        showStandardStreams = true
    }
    systemProperties = [
            'junit.platform.output.capture.stdout': 'true',
            'junit.platform.output.capture.stderr': 'true',
            'spark.version'                       :  sparkVersion,
            'openlineage.spark.jar'               : "${archivesBaseName}-${project.version}.jar",
            'kafka.package.version'               : "org.apache.spark:spark-sql-kafka-0-10_${versions.scala}:${sparkVersion}",
            'mockserver.logLevel'                 : 'ERROR'
    ]

    classpath = project.sourceSets.test.runtimeClasspath
}

// wrócić do jednego test z jakąś metodą zwracającą konfigurację żeby spark3 działał
test {
    useJUnitPlatform {i ->
        excludeTags ('integration-test')
        if(versions.delta == "NA") {excludeTags 'delta'}
        if(versions.iceberg == "NA") {excludeTags 'iceberg'}
    }
    configure commonTestConfiguration
    classpath = project.sourceSets.test.runtimeClasspath + configurations."${versions.module}"
}

task copyDependencies(type: Copy) {
    // delete the dependencies directory so we don't accidentally mix Spark 2 and Spark 3 dependencies
    duplicatesStrategy = DuplicatesStrategy.EXCLUDE
    delete layout.buildDirectory.dir("dependencies")
    def config = configurations."${versions.module}"
    from config.getFiles() + configurations.pysparkContainerOnly.getFiles()
    include "*.jar"
    into layout.buildDirectory.dir("dependencies")
}

task integrationTest(type: Test) {
    dependsOn shadowJar, copyDependencies
    retry {
        boolean isCiServer = System.getenv().containsKey("CI")
        if (isCiServer) {
            maxRetries = 3
            maxFailures = 3
        }
    }
    configure commonTestConfiguration
    useJUnitPlatform {
        includeTags "integration-test"
        excludeTags "databricks"
        if(!sparkVersion.startsWith('3')) {excludeTags 'spark3'}
        if(versions.delta == "NA") {excludeTags 'delta'}
        if(versions.iceberg == "NA") {excludeTags 'iceberg'}
    }
}

task databricksIntegrationTest(type: Test) {
    dependsOn shadowJar, copyDependencies
    retry {
        boolean isCiServer = System.getenv().containsKey("CI")
        if (isCiServer) {
            maxRetries = 3
            maxFailures = 3
        }
    }
    configure commonTestConfiguration
    useJUnitPlatform {
        includeTags "databricks"
    }
}

spotless {
    def disallowWildcardImports = {
        String text = it
        def regex = ~/import .*\.\*;/
        def m = regex.matcher(text)
        if (m.find()) {
            throw new AssertionError("Wildcard imports disallowed - ${m.findAll()}")
        }
    }
    java {
        googleJavaFormat()
        removeUnusedImports()
        custom 'disallowWildcardImports', disallowWildcardImports
    }
}

assemble {
    dependsOn shadowJar
}

shadowJar {
    dependsOn test

    minimize(){
        exclude (project(":shared"))
        exclude (project(":spark2"))
        exclude (project(":spark3"))
        exclude (project(":spark32"))
        exclude (project(":spark33"))
    }
    classifier = ''
    // avoid conflict with any client version of that lib
    relocate 'com.github.ok2c.hc5', 'io.openlineage.spark.shaded.com.github.ok2c.hc5'
    relocate 'org.apache.httpcomponents.client5', 'io.openlineage.spark.shaded.org.apache.httpcomponents.client5'
    relocate 'javassist', 'io.openlineage.spark.shaded.javassist'
    relocate 'org.apache.hc', 'io.openlineage.spark.shaded.org.apache.hc'
    relocate 'org.apache.commons.codec', 'io.openlineage.spark.shaded.org.apache.commons.codec'
    relocate 'org.apache.commons.logging', 'io.openlineage.spark.shaded.org.apache.commons.logging'
    relocate 'org.apache.commons.beanutils', 'io.openlineage.spark.shaded.org.apache.commons.beanutils'
    relocate 'org.apache.http', 'io.openlineage.spark.shaded.org.apache.http'
    relocate 'org.yaml.snakeyaml', 'io.openlineage.spark.shaded.org.yaml.snakeyaml'
    relocate('com.fasterxml.jackson', 'io.openlineage.spark.shaded.com.fasterxml.jackson') {
        exclude 'com.fasterxml.jackson.annotation.JsonIgnore'
        exclude 'com.fasterxml.jackson.annotation.JsonIgnoreProperties'
        exclude 'com.fasterxml.jackson.annotation.JsonIgnoreType'
    }
    manifest {
        attributes(
                'Created-By': "Gradle ${gradle.gradleVersion}",
                'Built-By': System.getProperty('user.name'),
                'Build-Jdk': System.getProperty('java.version'),
                'Implementation-Title': project.name,
                'Implementation-Version': project.version
        )
    }
    zip64 true
}

task createVersionProperties(dependsOn: processResources) {
    doLast {
        File dir = new File("$buildDir/resources/main/io/openlineage/spark/agent/")
        dir.mkdirs();
        new File("$buildDir/resources/main/io/openlineage/spark/agent/version.properties").withWriter { w ->
            Properties p = new Properties()
            p['version'] = project.version.toString()
            p.store w, null
        }
    }
}

classes {
    dependsOn createVersionProperties
}
