import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar

plugins {
    id("java-library")
    id("pmd")
    id("com.diffplug.spotless")
    id("io.freefair.lombok")
    id("com.github.johnrengelman.shadow")
    id("io.openlineage.common-config")
    id("io.openlineage.docker-build")
    id 'java-test-fixtures'
    id "com.adarshr.test-logger" version "3.2.0"
    id "org.gradle.test-retry" version "1.5.8"
}

configurations {
    spark2.extendsFrom testImplementation
    spark3.extendsFrom testImplementation
    spark31.extendsFrom testImplementation
    spark32.extendsFrom testImplementation
    spark33.extendsFrom testImplementation
    spark34.extendsFrom testImplementation
    spark35.extendsFrom testImplementation
    pysparkContainerOnly
}

archivesBaseName = 'openlineage-spark-app'

ext {
    assertjVersion = '3.25.1'
    bigqueryVersion = '0.29.0'
    postgresqlVersion = '42.7.1'
    mockitoVersion = '4.11.0'
    testcontainersVersion = '1.19.3'
    junit5Version = '5.10.1'

    sparkVersion = findProperty("spark.version")
    scalaBinaryVersion = findProperty("scala.binary.version")
    shortVersion = sparkVersion.substring(0, 3)

    sparkVersionFmt = sparkVersion.replace(".", "")
    scalaBinaryVersionFmt = scalaBinaryVersion.replace(".", "")
    dockerImageBuildTaskName = "buildDockerImageSpark${sparkVersionFmt}Scala${scalaBinaryVersionFmt}"

    versionsMap = [
            "3.5": ["module": "spark35", "scala": "2.12", "delta": "NA", "gcs": "hadoop3-2.2.9", "iceberg": "NA", "hadoopclient": "3.3.4"],
            "3.4": ["module": "spark34", "scala": "2.12", "delta": "2.4.0", "gcs": "hadoop3-2.2.9", "iceberg": "iceberg-spark-runtime-3.4_2.12:1.3.0", "hadoopclient": "3.3.4"],
            "3.3": ["module": "spark33", "scala": "2.12", "delta": "2.1.0", "gcs": "hadoop3-2.2.9", "iceberg": "iceberg-spark-runtime-3.3_2.12:0.14.0", "hadoopclient": "3.3.4"],
            "3.2": ["module": "spark32", "scala": "2.12", "delta": "1.1.0", "gcs": "hadoop3-2.2.9", "iceberg": "iceberg-spark-runtime-3.2_2.12:0.14.0", "hadoopclient": "3.3.4"],
            "3.1": ["module": "spark31", "scala": "2.12", "delta": "1.0.0", "gcs": "hadoop3-2.2.9", "iceberg": "iceberg-spark-runtime-3.1_2.12:0.13.0", "hadoopclient": "3.3.4"],
            "2.4": ["module": "spark2", "scala": "2.11", "delta": "NA", "gcs": "hadoop2-2.2.9", "iceberg": "NA", "hadoopclient": "2.10.2"]
    ]
    versions = versionsMap[shortVersion]

    destAdditionalJarsDir = layout.buildDirectory.dir("additional/jars")
    srcAdditionalConfDir = layout.projectDirectory.dir("additional/conf")
    destAdditionalConfDir = layout.buildDirectory.dir("additional/conf")

    scalaFmt = versions.scala.replace(".", "")
    scalaFixturesJarName = "openlineage-spark-scala-fixtures_${versions.scala}-${project.version}.jar"
    scalaFixturesConfiguration = "scala${scalaFmt}RuntimeElements"
}

// This workaround is needed because the version of Snappy that Spark 2.4.x runs with,
// cannot run on Apple Silicon. It fails with:
// org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] no native library is found for os.name=Mac and os.arch=aarch64
configurations.configureEach {
    resolutionStrategy.eachDependency { DependencyResolveDetails details ->
        if (details.requested.group == "org.xerial.snappy" && details.requested.name == "snappy-java") {
            details.useVersion("[1.1.8.4,)")
        }
    }
}

configurations {
    // This configuration holds JARs that will be mounted to the Docker container and then executed.
    // For example, a Spark job that is written in Java or Scala and is executed using the spark-submit command.
    integrationTestFixtures {
        canBeResolved = true
        canBeConsumed = false
    }

    // This configuration holds JARs that will be mounted in order to support the execution of an integration test.
    // For example, running the Iceberg integration tests requires the Iceberg JARs to be present on the classpath.
    // This configuration facilitates that.
    additionalJars {
        canBeResolved = true
        canBeConsumed = false
    }
}

dependencies {
    implementation(project(path: ":shared"))
    implementation(project(path: ":spark2"))
    implementation(project(path: ":spark3"))
    implementation(project(path: ":spark31"))
    implementation(project(path: ":spark32"))
    implementation(project(path: ":spark33"))
    implementation(project(path: ":spark34"))
    implementation(project(path: ":spark35"))
    implementation 'org.apache.httpcomponents.client5:httpclient5:5.3'

    compileOnly "org.apache.spark:spark-core_${versions.scala}:${sparkVersion}"
    compileOnly "org.apache.spark:spark-sql_${versions.scala}:${sparkVersion}"
    compileOnly("com.google.cloud.spark:spark-bigquery-with-dependencies_${versions.scala}:${bigqueryVersion}") {
        exclude group: 'com.fasterxml.jackson.core'
        exclude group: 'com.fasterxml.jackson.module'
        exclude group: 'com.sun.jmx'
        exclude group: 'com.sun.jdmk'
        exclude group: 'javax.jms'
    }


    testFixturesApi platform('org.junit:junit-bom:5.10.1')
    testFixturesApi "org.junit.jupiter:junit-jupiter:${junit5Version}"
    testFixturesApi "org.junit.jupiter:junit-jupiter-params:${junit5Version}"

    testFixturesApi "org.postgresql:postgresql:${postgresqlVersion}"
    testFixturesApi 'org.hamcrest:hamcrest-library:2.2'
    testFixturesApi 'org.xerial:sqlite-jdbc:3.45.0.0'
    testFixturesApi "org.testcontainers:junit-jupiter:${testcontainersVersion}"
    testFixturesApi "org.testcontainers:postgresql:${testcontainersVersion}"
    testFixturesApi "org.testcontainers:mockserver:${testcontainersVersion}"
    testFixturesApi "org.testcontainers:kafka:${testcontainersVersion}"

    testFixturesApi "org.apache.spark:spark-core_${versions.scala}:${sparkVersion}"
    testFixturesApi "org.apache.spark:spark-sql_${versions.scala}:${sparkVersion}"
    testFixturesApi "org.apache.spark:spark-hive_${versions.scala}:${sparkVersion}"
    testFixturesApi("org.apache.spark:spark-mllib_${versions.scala}:${sparkVersion}") {
        exclude group: 'org.glassfish.jaxb', module: 'jaxb-runtime'
        exclude group: "org.apache.spark", module: "spark-graphx_${versions.scala}"
    }
    testFixturesApi 'commons-beanutils:commons-beanutils:1.9.4'
    if (versions.module != "spark2") {
        testFixturesApi("org.apache.spark:spark-hadoop-cloud_${versions.scala}:${sparkVersion}") {
            exclude group: 'com.fasterxml.jackson.core'
            exclude group: 'org.apache.hadoop', module: 'hadoop-azure'
            exclude group: 'org.apache.hadoop', module: 'hadoop-openstack'
        }
    }


    testFixturesApi "org.apache.spark:spark-sql-kafka-0-10_${versions.scala}:${sparkVersion}"
    testFixturesApi("com.google.cloud.spark:spark-bigquery-with-dependencies_${versions.scala}:${bigqueryVersion}") {
        exclude group: 'com.fasterxml.jackson.core'
        exclude group: 'com.fasterxml.jackson.module'
    }
    testFixturesApi("com.databricks:databricks-sdk-java:0.4.0") {
        exclude group: 'com.fasterxml.jackson.core'
        exclude group: 'com.fasterxml.jackson.module'
    }

    testFixturesApi("org.apache.hadoop:hadoop-client:${versions.hadoopclient}") {
        exclude group: 'org.apache.hadoop', module: 'hadoop-hdfs-client'
        exclude group: 'org.apache.hadoop', module: 'hadoop-client'
        exclude group: 'org.apache.hadoop', module: 'hadoop-mapreduce-client-core'
        exclude group: 'org.apache.hadoop', module: 'hadoop-yarn-common'
        exclude group: 'com.fasterxml.jackson.core'
    }
    testFixturesApi("org.mock-server:mockserver-netty:5.14.0:shaded") {
        exclude group: 'com.google.guava', module: 'guava'
        exclude group: 'com.fasterxml.jackson.core'
        exclude group: 'com.fasterxml.jackson.datatype'
        exclude group: 'com.fasterxml.jackson.dataformat'
        exclude group: 'org.mock-server.mockserver-client-java'
    }

    testFixturesApi group: 'org.awaitility', name: 'awaitility', version: '4.2.0'
    testFixturesApi "org.assertj:assertj-core:${assertjVersion}"
    testFixturesApi "org.mockito:mockito-core:${mockitoVersion}"
    testFixturesApi "org.mockito:mockito-inline:${mockitoVersion}"
    testFixturesApi "org.mockito:mockito-junit-jupiter:${mockitoVersion}"

    pysparkContainerOnly("com.google.cloud.bigdataoss:gcs-connector:${versions.gcs}:shaded") {
        exclude group: 'com.google.guava', module: 'guava'
    }
    pysparkContainerOnly("com.google.guava:guava:30.1-jre")

    if (versions.delta != "NA") {
        testFixturesApi "io.delta:delta-core_${versions.scala}:${versions.delta}"
    }

    if (versions.iceberg != "NA") {
        testFixturesApi "org.apache.iceberg:${versions.iceberg}"
    }

    if (versions.module != "spark2") {
        testImplementation("com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.9") {
            exclude group: 'com.google.guava', module: 'guava'
        }
    }

    integrationTestFixtures(project(path: ":scala-fixtures", configuration: scalaFixturesConfiguration))

    if (versions.module == "spark35") {
        // We need to upload logging JARS, for some reason
        additionalJars("org.slf4j:slf4j-api:2.0.10")
        additionalJars("org.slf4j:slf4j-reload4j:2.0.10")
    }
}

def commonTestConfiguration = {
    forkEvery 1
    maxParallelForks 5
    testLogging {
        events "passed", "skipped", "failed"
        showStandardStreams = true
    }
    systemProperties = [
            "additional.conf.dir"                 : destAdditionalConfDir.get().asFile.absolutePath,
            "additional.jars.dir"                 : destAdditionalJarsDir.get().asFile.absolutePath,
            "derby.system.home.base"              : layout.buildDirectory.dir("var/run/derby/${versions.module}").get().asFile.absolutePath,
            "fixtures.dir"                        : layout.buildDirectory.dir("fixtures").get().asFile.absolutePath,
            "junit.platform.output.capture.stderr": "true",
            "junit.platform.output.capture.stdout": "true",
            "kafka.package.version"               : "org.apache.spark:spark-sql-kafka-0-10_${versions.scala}:${sparkVersion}",
            "lib.dir"                             : layout.buildDirectory.dir("lib").get().asFile.absolutePath,
            "mockserver.logLevel"                 : "ERROR",
            "openlineage.spark.jar"               : tasks.named("shadowJar", ShadowJar.class).get().outputs.files.singleFile.name,
            "resources.dir"                       : layout.buildDirectory.dir("resources/test").get().asFile.absolutePath,
            "scala.fixtures.jar.name"             : scalaFixturesJarName.toString(),
            "spark.docker.image"                  : "bitnami/spark:${sparkVersion}",
            "spark.home.dir"                      : "/opt/bitnami/spark",
            "spark.sql.warehouse.dir"             : layout.buildDirectory.dir("var/run/spark-warehouse/${versions.module}").get().asFile.absolutePath,
            "spark.version"                       : sparkVersion,
    ]

    classpath = project.sourceSets.test.runtimeClasspath
}

if (sparkVersion == '2.4.6') {
    tasks.register("buildDockerImage") {
        dependsOn()
        group = "docker"
        description = "This is a no-op, because the manifest file is configured to build an image for Spark 2.4.8, and " +
                "the Apache Spark folks decided not to include the Hadoop JARs with the 2.12 variant of 2.4.8. " +
                "I'm not sure why. Any way this means we don't build this image, for the time being. This will be fixed in " +
                "the future. At the same time, Bitnami never produced a Spark 2.4.8 image."
        doLast {
            println("This task is a no-op. See the task description for more information.")
        }
    }
} else {
    tasks.register("buildDockerImage") {
        dependsOn(tasks.named(dockerImageBuildTaskName))
        group = "docker"
        description = "Builds the docker image specified by the properties 'spark.version' and 'scala.binary.version'"
    }
}


// These entries aren't considered "outputs" of any Gradle task, thus Gradle doesn't know about
// them and won't clean them up when you run './gradlew clean'. This is a problem because
// you may experience weird test failures, when running the tests using different versions of
// Apache Spark. Thus this take is a workaround to clean up these file(s) and folder(s).
tasks.register("cleanUp", Delete) {
    delete(layout.buildDirectory.dir("var/run/derby/${versions.module}"))
    delete(layout.buildDirectory.dir("var/run/spark-warehouse/${versions.module}"))
    // some tests seem to create these directories in the 'app' directory,
    // instead of using the build dir. This makes it hard to prevent conflicts (particularly in the metastore)
    // when you run the tests with different versions of Apache Spark on the same machine.
    delete("derby.log", "metastore_db", "spark-warehouse")
}

tasks.withType(Test).configureEach {
    failFast = true
}

tasks.register("copyIntegrationTestFixtures", Copy) {
    group = "copy"
    description = "Copies dependencies in the integrationTestFixtures configuration to the build directory"
    from(configurations.integrationTestFixtures)
    into(layout.buildDirectory.dir("fixtures"))
}

tasks.register("copyAdditionalJars", Copy) {
    group = "copy"
    description = "Copies dependencies in the 'additionalJars' configuration to the build directory"
    from(configurations.additionalJars)
    into(destAdditionalJarsDir)

    it.onlyIf({ !configurations.additionalJars.getFiles().any() })
}

tasks.register("copyAdditionalConfiguration", Copy) {
    group = "copy"
    description = "Copies additional spark configuration files to the build directory"
    from(srcAdditionalConfDir)
    into(destAdditionalConfDir)
}

// wrócić do jednego test z jakąś metodą zwracającą konfigurację żeby spark3 działał
tasks.named("test", Test).configure {
    dependsOn(tasks.named("cleanUp"))
    useJUnitPlatform {
        excludeTags("integration-test")
        if (versions.delta == "NA") {
            excludeTags "delta"
        }
        if (versions.iceberg == "NA") {
            excludeTags "iceberg"
        }
    }
    configure commonTestConfiguration
    classpath = project.sourceSets.test.runtimeClasspath + configurations."${versions.module}"
}

tasks.register("copyDependencies", Copy) {
    // TODO: We need to fix this. Copy dependencies shouldn't depend on 'jar' and 'testFixturesJar'
    mustRunAfter(tasks.named("jar"), tasks.named("testFixturesJar"))
    dependsOn(tasks.named("shadowJar"))
    // delete the dependencies directory so we don"t accidentally mix Spark 2 and Spark 3 dependencies
    duplicatesStrategy = DuplicatesStrategy.EXCLUDE
    delete layout.buildDirectory.dir("dependencies")
    def config = configurations."${versions.module}"
    from config.getFiles() + configurations.pysparkContainerOnly.getFiles()
    include "*.jar"
    into layout.buildDirectory.dir("dependencies")
}

def integrationTestDependencies = [
        tasks.named("shadowJar"),
        tasks.named("copyDependencies"),
        tasks.named("copyIntegrationTestFixtures"),
        tasks.named("copyAdditionalJars"),
        tasks.named("copyAdditionalConfiguration")
]

tasks.register("integrationTest", Test) {
    group = "verification"

    dependsOn(integrationTestDependencies)
    retry {
        boolean isCiServer = System.getenv().containsKey("CI")
        if (isCiServer) {
            maxRetries = 3
            maxFailures = 3
        }
    }
    configure commonTestConfiguration

    useJUnitPlatform {
        includeTags "integration-test"
        excludeTags "databricks"
        if (versions.delta == "NA") {
            excludeTags "delta"
        }
        if (versions.iceberg == "NA") {
            excludeTags "iceberg"
        }
    }
}

tasks.register("databricksIntegrationTest", Test) {
    dependsOn(integrationTestDependencies)
    retry {
        boolean isCiServer = System.getenv().containsKey("CI")
        if (isCiServer) {
            maxRetries = 3
            maxFailures = 3
        }
    }
    configure commonTestConfiguration
    useJUnitPlatform {
        includeTags "databricks"
    }
}

assemble {
    dependsOn shadowJar
}

tasks.named("shadowJar", ShadowJar.class) {
    dependsOn(tasks.named("jar"))
    group = "shadow"
    destinationDirectory.set(layout.buildDirectory.dir("libs/shadow"))
    minimize() {
        exclude(project(":shared"))
        exclude(project(":spark2"))
        exclude(project(":spark3"))
        exclude(project(":spark31"))
        exclude(project(":spark32"))
        exclude(project(":spark33"))
        exclude(project(":spark34"))
        exclude(project(":spark35"))
    }
    archiveClassifier = ''
    // avoid conflict with any client version of that lib
    relocate 'com.github.ok2c.hc5', 'io.openlineage.spark.shaded.com.github.ok2c.hc5'
    relocate 'org.apache.httpcomponents.client5', 'io.openlineage.spark.shaded.org.apache.httpcomponents.client5'
    relocate 'javassist', 'io.openlineage.spark.shaded.javassist'
    relocate 'org.apache.hc', 'io.openlineage.spark.shaded.org.apache.hc'
    relocate 'org.apache.commons.codec', 'io.openlineage.spark.shaded.org.apache.commons.codec'
    relocate 'org.apache.commons.lang3', 'io.openlineage.spark.shaded.org.apache.commons.lang3'
    relocate 'org.apache.commons.beanutils', 'io.openlineage.spark.shaded.org.apache.commons.beanutils'
    relocate 'org.apache.http', 'io.openlineage.spark.shaded.org.apache.http'
    relocate 'org.yaml.snakeyaml', 'io.openlineage.spark.shaded.org.yaml.snakeyaml'
    relocate 'org.slf4j', 'io.openlineage.spark.shaded.org.slf4j'
    relocate('com.fasterxml.jackson', 'io.openlineage.spark.shaded.com.fasterxml.jackson') {
        exclude 'com.fasterxml.jackson.annotation.JsonIgnore'
        exclude 'com.fasterxml.jackson.annotation.JsonIgnoreProperties'
        exclude 'com.fasterxml.jackson.annotation.JsonIgnoreType'
    }
    manifest {
        attributes(
                'Created-By': "Gradle ${gradle.gradleVersion}",
                'Built-By': System.getProperty('user.name'),
                'Build-Jdk': System.getProperty('java.version'),
                'Implementation-Title': project.name,
                'Implementation-Version': project.version
        )
    }
    zip64 true
}

tasks.register("createVersionProperties") {
    dependsOn(tasks.named("processResources"))
    doLast {
        def dir = layout.buildDirectory.dir("resources/main/io/openlineage/spark/agent").get()
        dir.asFile.mkdirs()
        dir.file("version.properties").asFile.withWriter { w ->
            Properties p = new Properties()
            p["version"] = project.version.toString()
            p.store w, null
        }
    }
}

tasks.named("classes") {
    dependsOn(tasks.named("createVersionProperties"))
}

tasks.named("pmdTest") {
    mustRunAfter("shadowJar")
}
