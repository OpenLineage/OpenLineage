# Hive OpenLineage Integration

This project provides an [Apache Hive](https://hive.apache.org/) integration for
[OpenLineage](https://openlineage.io/), enabling automated data lineage capture
for your Hive workloads. It dynamically analyzes Hive query plans, extracts
dependencies between datasets, and generates lineage metadata in the OpenLineage
format. This metadata can then be used for various purposes, such as data
discovery, impact analysis, and data governance. You can further propagate these
OpenLineage events to [Google Cloud Dataplex](https://cloud.google.com/dataplex),
enhancing your data lineage capabilities within the Google Cloud ecosystem.

## Supported Query Types

This integration supports a wide range of Hive query types, including:

* `CREATE TABLE AS SELECT` (`CTAS`): Captures lineage from source tables to the
  newly created table. Includes operations like `SELECT`, `JOIN`, `WHERE`
  filters, and aggregations within the `CTAS` statement.
* `INSERT` (`OVERWRITE TABLE` | `INTO TABLE`): Captures lineage from source data
  to the destination table. Includes operations like `SELECT`, `JOIN`, `WHERE`
  filters, and aggregations within the `INSERT` statement.
* `SELECT` statements: Do not emit lineage events on their own (as they don't
  change data). However, intermediate transformations within a `SELECT` used in
  a `CTAS` or `INSERT` are analyzed for column-level lineage.
* Complex Queries: Supports complex queries involving Common Table Expressions
  (CTEs), joins, filters, aggregations, sorting, window functions, and more.
* Union statements: `UNION ALL` statements are supported capturing lineage from
  multiple input tables to a single destination.

## How It Works

* Hive Hook: The core of the integration is a Hive execution hook
  (`HiveOpenLineageHook`) that intercepts query execution.
* Query Plan Analysis: The hook analyzes the Hive query plan generated by the
  SemanticAnalyzer. It traverses the plan's Abstract Syntax Tree (AST) to
  identify input and output datasets, as well as the transformations performed
  on the data. It leverages a custom parser (separate from Hive's parser) for
  more advanced column-level lineage analysis.
* OpenLineage Event Generation: Based on the query plan analysis, the hook
  constructs OpenLineage events, capturing the data lineage information. Events
  include details about the job, datasets (inputs and outputs), and the
  relationships between them.  The resulting OpenLineage event will be of type
  `COMPLETE` for successful queries and `FAIL` for failed queries.
* Transport: The events are then sent to the configured transport.
* Dataplex Integration (Optional): If configured with the `gcplineage`
  transport, the OpenLineage events are published to Dataplex, creating a
  comprehensive view of your data lineage within your Google Cloud environment.
 
## Configuration

The integration is configured using Hive configuration properties. The most
relevant configuration options include:

* `hive.openlineage.transport.type`: Specifies the OpenLineage
  [transport](https://openlineage.io/docs/integrations/spark/configuration/transport/)
  mechanism for OpenLineage events. For example, set to `composite` to enable
  sending events to multiple transports simultaneously, or to `gcplineage` for
  Google Cloud Dataplex.
* `hive.openlineage.namespace`: Defines the namespace for the OpenLineage job.
* `hive.openlineage.job.name`: Specifies the name of the OpenLineage job. Useful
  for filtering data in your lineage visualization tool
* `hive.exec.post.hooks` and `hive.exec.failure.hooks`:  The Hive hook can be
  enabled for both successful and failed queries. This is configured using the
  following Hive properties:
  ```properties
  hive.exec.post.hooks=io.openlineage.hive.hooks.HiveOpenLineageHook
  hive.exec.failure.hooks=io.openlineage.hive.hooks.HiveOpenLineageHook
  ```

## Getting started

Create a Dataproc cluster:

```shell
gcloud dataproc clusters create ${CLUSTER_NAME} \
  --zone ${ZONE} \
  --region ${REGION} \
  --image-version 2.1-debian11 \
  --scopes cloud-platform \
  --service-account "${SERVICE_ACCOUNT_NAME}@${PROJECT}.iam.gserviceaccount.com" \
  --image-version 2.1-debian11 \
  --initialization-actions gs://hive-lineage-dev-releases/install_jars.sh \
  --metadata "jar-urls=gs://hive-lineage-dev-releases/hive-openlineage-hook-shaded_2024-12-08.jar" \
  --properties "hive:hive.exec.post.hooks=io.openlineage.hive.hooks.HiveOpenLineageHook" \
  --properties "hive:hive.conf.validation=false" \
  --properties "hive:hive.openlineage.namespace=mynamespace" \
  --properties "hive:hive.openlineage.transport.type=gcplineage" \
  --properties "hive:hive.openlineage.transport.projectId=${PROJECT}" \
  --properties "hive:hive.openlineage.transport.location=us"
```

Create a sample source table:

```shell
gcloud dataproc jobs submit hive \
    --cluster ${CLUSTER_NAME} \
    --region ${REGION} \
    --execute "
      DROP TABLE IF EXISTS transactions;
      CREATE EXTERNAL TABLE transactions
      (SubmissionDate DATE, TransactionAmount DOUBLE, TransactionType STRING)
      "
```

Create a sample destination table:

```shell
gcloud dataproc jobs submit hive \
  --cluster ${CLUSTER_NAME} \
  --region ${REGION} \
  --properties "hive.openlineage.job.name=monthly_transaction_summary_job" \
  --execute "
DROP TABLE IF EXISTS monthly_transaction_summary;
CREATE TABLE monthly_transaction_summary
AS
SELECT
    TRUNC(submissiondate, 'MM') AS Month,
    transactiontype,
    SUM(transactionamount) AS TotalAmount,
    COUNT(*) AS TransactionCount
FROM
    transactions
GROUP BY
    TRUNC(submissiondate, 'MM'),
    transactiontype
ORDER BY
    Month,
    transactiontype;"
```

The last query above should generate a lineage event describing the data flow
from the `transactions` table to the `monthly_transaction_summary` table.